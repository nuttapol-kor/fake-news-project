{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05UkpfT4oeTF",
        "outputId": "c3797aff-9c42-4b38-c622-dd1678ff9f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Collecting pythainlp\n",
            "  Downloading pythainlp-3.1.1-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.8/dist-packages (from pythainlp) (2.25.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=ade6dc293bd03617a61c9b6a8406ce48d17a0a2015cf53bab1ee05061de2e7df\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, pythainlp\n",
            "Successfully installed emoji-2.2.0 pythainlp-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn pythainlp emoji tensorflow numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r_rITMa0ocnK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import unicodedata\n",
        "import pythainlp\n",
        "import re\n",
        "import emoji\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6syFu2z9ocnN"
      },
      "outputs": [],
      "source": [
        "def normalize_double_quote(text: str):\n",
        "    all_quote = [\n",
        "    '“', # U+201c\n",
        "    '”', # U+201d\n",
        "    \"„\", # U+201e\n",
        "    '«', # U+00AB\n",
        "    '»', # U+00BB\n",
        "    '„', # U+201E\n",
        "    '“', # U+201C\n",
        "    '‟', # U+201F\n",
        "    '”', # U+201D\n",
        "    '❝', # U+275D\n",
        "    '❞', # U+275E\n",
        "    '〝', # U+301D\n",
        "    '〞', # U+301E\n",
        "    '〟', # U+301F\n",
        "    '＂', # U+FF02\n",
        "    ]\n",
        "    std_quote = \"\\\"\" #U+0022\n",
        "    table = str.maketrans(dict.fromkeys(all_quote, std_quote))\n",
        "    return text.translate(table)\n",
        "\n",
        "def normalize_single_quote(text: str):\n",
        "    all_quote = ['\\u02BB', '\\u02BC', '\\u066C', '\\u2018', '\\u2019', '\\u201A', '\\u275B', '\\u275C']\n",
        "    std_quote = \"\\'\"\n",
        "    table = str.maketrans(dict.fromkeys(all_quote, std_quote))\n",
        "    return text.translate(table)\n",
        "\n",
        "def replace_url(text):\n",
        "    URL_PATTERN = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\n",
        "]+|\n",
        "[^\\s()]*?\\)|\n",
        ")+(?:\n",
        "[^\\s()]*?\\)|\n",
        "|[^\\s`!()\n",
        "{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
        "    return re.sub(URL_PATTERN, 'xxurl', text)\n",
        "\n",
        "def replace_rep(text):\n",
        "    def _replace_rep(m):\n",
        "        c,cc = m.groups()\n",
        "        return f'{c}xxrep'\n",
        "    re_rep = re.compile(r'(\\S)(\\1{2,})')\n",
        "    return re_rep.sub(_replace_rep, text)\n",
        "\n",
        "def ungroup_emoji(toks):\n",
        "    res = []\n",
        "    for tok in toks:\n",
        "        if emoji.emoji_count(tok) == len(tok):\n",
        "            for char in tok:\n",
        "                res.append(char)\n",
        "        else:\n",
        "            res.append(tok)\n",
        "    return res\n",
        "\n",
        "def preprocess_func(text):\n",
        "    new_text = unicodedata.normalize('NFKD', text)\n",
        "    new_text = pythainlp.util.normalize(new_text)\n",
        "    new_text = normalize_double_quote(new_text)\n",
        "    new_text = normalize_single_quote(new_text)\n",
        "\n",
        "    new_text = new_text.lower().strip()\n",
        "    new_text = replace_url(new_text)\n",
        "    new_text = replace_rep(new_text)\n",
        "\n",
        "    res = [word for word in pythainlp.word_tokenize(new_text) if word and not re.search(pattern=r\"\\s+\", string=word)]\n",
        "\n",
        "    res = ungroup_emoji(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PQ7Yhda6ocnO"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('./train_data_fake_news.csv')\n",
        "val_df = pd.read_csv('./valid_data_fake_news.csv')\n",
        "test_df = pd.read_csv('./test_data_fake_news.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eSbij9OdocnP"
      },
      "outputs": [],
      "source": [
        "text_train = [' '.join(preprocess_func(sent)) for sent in train_df['text']]\n",
        "text_val = [' '.join(preprocess_func(sent)) for sent in val_df['text']]\n",
        "text_test = [' '.join(preprocess_func(sent)) for sent in test_df['text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QdrEh3NkocnP"
      },
      "outputs": [],
      "source": [
        "word_count = []\n",
        "for sent in text_train:\n",
        "    for word in sent.split():\n",
        "        word_count.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YrW_aGgQocnP"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(set(word_count))\n",
        "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(text_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9quiKbCXocnQ",
        "outputId": "dbffa739-fdbd-42f4-bb40-c0c22a562634"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25273"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x9JhqDY2ocnQ"
      },
      "outputs": [],
      "source": [
        "y_train = train_df['label'].astype(str)\n",
        "y_val = val_df['label'].astype(str)\n",
        "y_test = test_df['label'].astype(str)\n",
        "y_class = ['0', '1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SA-j1VlocnR",
        "outputId": "bfa06487-cfa4-44b3-99ab-84677db167af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       1\n",
              "1       1\n",
              "2       1\n",
              "3       0\n",
              "4       1\n",
              "       ..\n",
              "2358    1\n",
              "2359    1\n",
              "2360    0\n",
              "2361    0\n",
              "2362    1\n",
              "Name: label, Length: 2363, dtype: object"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7rjUdiUocnR",
        "outputId": "9d6ff816-64fe-4974-e31a-7685f8650767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2363, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_class)\n",
        "y_train = le.transform(y_train)\n",
        "y_val = le.transform(y_val)\n",
        "y_test = le.transform(y_test)\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yhd-2yBocnS",
        "outputId": "761552e7-9bbf-4778-a9aa-fd09223d0547"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['label'].values.tolist()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t77QGrBFocnS",
        "outputId": "bef74093-0c50-42aa-bae7-b357c7a945e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 1.], dtype=float32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NDLSubDvocnS"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=512,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCfOdAxtocnS",
        "outputId": "0b4450c4-a134-4ef2-d074-4c62ec6384cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 512)         12273152  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 512)              1574912   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,913,986\n",
            "Trainable params: 13,913,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvkQ5rTWocnT",
        "outputId": "fffdb2d0-d1a3-475b-9a25-4df039f11ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "[[0.5014064  0.49859357]]\n"
          ]
        }
      ],
      "source": [
        "sample_text = ('ร้านอาหารร้านนี้อร่อยมาก')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uvmEqBkKocnT"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-p9Q-9locnT",
        "outputId": "0c736ad3-9e27-48b5-c8fd-0aad3efd514d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "74/74 - 1753s - loss: 0.2819 - accuracy: 0.9099 - val_loss: 0.2993 - val_accuracy: 0.8874 - 1753s/epoch - 24s/step\n",
            "Epoch 2/5\n",
            "74/74 - 1750s - loss: 0.1803 - accuracy: 0.9424 - val_loss: 0.2787 - val_accuracy: 0.8972 - 1750s/epoch - 24s/step\n",
            "Epoch 3/5\n",
            "74/74 - 1790s - loss: 0.1029 - accuracy: 0.9687 - val_loss: 0.3175 - val_accuracy: 0.8893 - 1790s/epoch - 24s/step\n",
            "Epoch 4/5\n",
            "74/74 - 1791s - loss: 0.0579 - accuracy: 0.9865 - val_loss: 0.3183 - val_accuracy: 0.8953 - 1791s/epoch - 24s/step\n",
            "Epoch 5/5\n",
            "74/74 - 1757s - loss: 0.0307 - accuracy: 0.9928 - val_loss: 0.3805 - val_accuracy: 0.8992 - 1757s/epoch - 24s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffaef6b2640>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.keras import TqdmCallback\n",
        "model.fit(np.array(text_train), y_train, validation_data=(np.array(text_val), y_val), epochs=5, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76JwyWa9hP36",
        "outputId": "557a8a67-7c80-4cf2-c0c1-69100353a03c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "model.save(\"rnn_lstm_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpaJYLzrh-k-",
        "outputId": "f43a6784-fd9e-4d97-f2e7-3eb09cfd26f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/rnn_lstm_model/ (stored 0%)\n",
            "  adding: content/rnn_lstm_model/saved_model.pb (deflated 88%)\n",
            "  adding: content/rnn_lstm_model/assets/ (stored 0%)\n",
            "  adding: content/rnn_lstm_model/variables/ (stored 0%)\n",
            "  adding: content/rnn_lstm_model/variables/variables.data-00000-of-00001 (deflated 6%)\n",
            "  adding: content/rnn_lstm_model/variables/variables.index (deflated 65%)\n",
            "  adding: content/rnn_lstm_model/keras_metadata.pb (deflated 89%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r '/content/rnn_lstm_model.zip\"' '/content/rnn_lstm_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFiKamc_ocnT",
        "outputId": "d4d63f23-0134-4aa1-951b-d2c536df5b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 67s 4s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.86       212\n",
            "           1       0.90      0.90      0.90       295\n",
            "\n",
            "    accuracy                           0.88       507\n",
            "   macro avg       0.88      0.88      0.88       507\n",
            "weighted avg       0.88      0.88      0.88       507\n",
            "\n"
          ]
        }
      ],
      "source": [
        "value = model.predict(np.array(text_test))\n",
        "y_test_pred = np.argmax(value,axis=1)\n",
        "y_test_true = np.argmax(y_test,axis=1)\n",
        "print(classification_report(y_test_true, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "W6wbQyS6mk5p"
      },
      "outputs": [],
      "source": [
        "loaded_model = tf.keras.models.load_model(\"./rnn_lstm_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlul4lUinDzN",
        "outputId": "0336dfd6-86fc-4f9e-fab0-de07895a0892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 68s 4s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.86       212\n",
            "           1       0.90      0.90      0.90       295\n",
            "\n",
            "    accuracy                           0.88       507\n",
            "   macro avg       0.88      0.88      0.88       507\n",
            "weighted avg       0.88      0.88      0.88       507\n",
            "\n"
          ]
        }
      ],
      "source": [
        "value = loaded_model.predict(np.array(text_test))\n",
        "y_test_pred = np.argmax(value,axis=1)\n",
        "y_test_true = np.argmax(y_test,axis=1)\n",
        "print(classification_report(y_test_true, y_test_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b5611d40d9a35cece57bb11b4fe7cfb894178ef53f2345e73f0f231f12336cee"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
